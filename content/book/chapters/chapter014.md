---
title: "Chapter 14: Conditional Expectation and Orthogonal Projection"
layout: "single"
url: "/book/chapters/chapter014/"
summary: "Conditional expectation as a Radonâ€“Nikodym derivative on an information horizon; MSE minimization as orthogonal projection in LÂ²; attention, denoising, and regression as instances of the same projection theorem."
draft: false
ShowToc: true
TocOpen: true
hideMeta: true
math: true
weight: 14
---

<style>
  .post-content,
  .post-content h1, .post-content h2, .post-content h3, .post-content h4,
  .post-content p, .post-content li, .post-content blockquote,
  .post-content td, .post-content th {
    font-family: 'Times New Roman', 'Times', 'Noto Serif', Georgia, serif;
  }
  .post-content { font-size: 12pt; line-height: 1.72; }
  .def-box  { border-left:4px solid #4a148c; background:#faf5ff; padding:1em 1.2em; margin:1.5em 0; border-radius:4px; }
  .prop-box { border-left:4px solid #1565c0; background:#f0f6ff; padding:1em 1.2em; margin:1.5em 0; border-radius:4px; }
  .proof-box{ border-left:4px solid #999;    background:#fafafa; padding:.8em 1.2em; margin:1em 0 1.5em; border-radius:4px; }
  .ml-box   { border-left:4px solid #e65100; background:#fff8f0; padding:.8em 1.2em; margin:1em 0 1.5em; border-radius:4px; }
  .scholium-box { border:2px solid #6a1b9a; background:#fdf5ff; padding:1em 1.2em; margin:1.5em 0; border-radius:6px; }
</style>

<div style="text-align:center; margin:1.5em 0 2.5em 0;">

# Volume I &mdash; Mathematical Foundations and Axiomatization

## Part B &mdash; Probability &amp; Measure

## Chapter 14 &mdash; Conditional Expectation and Orthogonal Projection: The Geometry of Prediction Under Information Constraints

*Xujiang Tang*

</div>

## Abstract

Conditional expectation is the central object of statistical prediction. Chapter 13 showed that density ratios govern the comparison of measures; Chapter 14 shows that *averaging under an information constraint* â€?the operation underlying regression, denoising, attention, and policy evaluation â€?is a Radonâ€“Nikodym derivative on a restricted \(\sigma\)-algebra, and simultaneously the orthogonal projection in the Hilbert space \(L^2\). These two characterizations are equivalent and together explain why MSE minimization, the Pythagorean biasâ€“variance identity, and the optimality of softmax attention are not separate empirical findings but corollaries of a single geometric theorem.

---

## 14.1 The Problem: What Does It Mean to "Predict" Under Limited Information?

A machine learning model observes a feature vector \(X\) (pixels, tokens, sensor readings) and must produce a prediction of a target \(Y\). The model's information is limited to the \(\sigma\)-algebra generated by its inputs. Two fundamental questions arise:

1. What is the *best possible* prediction, given that information limit?
2. In what sense is training with a loss function an approximation to that best prediction?

The answer to both questions is "conditional expectation," but the phrase must be given its full measure-theoretic content. This chapter develops that content and shows how it unifies the theory behind regression, denoising diffusion, attention mechanisms, and Bregman projections.

---

## 14.2 Sub-\(\sigma\)-Algebras as Information Horizons

<div class="def-box">

**Definition 14.1 (information horizon).** Let \((\Omega,\mathcal{F},\mathbb{P})\) be a probability space. A *sub-\(\sigma\)-algebra* \(\mathcal{G}\subseteq\mathcal{F}\) encodes a coarser level of information: an event \(A\in\mathcal{G}\) is observable at the \(\mathcal{G}\)-horizon, while events in \(\mathcal{F}\setminus\mathcal{G}\) are invisible. A random variable \(Z\) is \(\mathcal{G}\)-measurable if and only if it depends only on information in \(\mathcal{G}\).

</div>

Standard ML instances:
- \(\mathcal{G} = \sigma(X)\): information generated by the feature vector \(X\).
- \(\mathcal{G} = \sigma(h(X))\): information generated by a learned representation \(h\).
- \(\mathcal{G} = \sigma(X_t)\): information generated by a noisy/corrupted version \(X_t\) of clean data \(X_0\).

The key point: any \(\mathcal{G}\)-measurable predictor is *constrained* by what \(\mathcal{G}\) can see. Sharpness beyond the information horizon is measurability-illegal.

---

## 14.3 Existence via Radonâ€“Nikodym: Conditional Expectation Is a Derivative on an Information Horizon

> **Theorem-sentence 14.1.** For any integrable random variable \(Y\in L^{1}(\Omega,\mathcal{F},\mathbb{P})\) and any sub-\(\sigma\)-algebra \(\mathcal{G}\subseteq\mathcal{F}\), the conditional expectation \(\mathbb{E}[Y\mid\mathcal{G}]\) exists and is uniquely determined \(\mathbb{P}\)-a.s. as a Radonâ€“Nikodym derivative.

### 14.3.1 Construction (measure-theoretic, canonical)

Fix \(\mathcal{G}\subseteq\mathcal{F}\). Define a signed measure \(\mu\) on \((\Omega,\mathcal{G})\) by
\[
\mu(A) := \int_A Y\, d\mathbb{P} = \mathbb{E}[Y\mathbf{1}_A], \qquad A\in\mathcal{G}.
\]
Then \(\mu\) is \(\sigma\)-additive (linearity of the Lebesgue integral) and is absolutely continuous with respect to \(\mathbb{P}\) restricted to \(\mathcal{G}\): if \(\mathbb{P}(A)=0\), then \(\mu(A)=\mathbb{E}[Y\mathbf{1}_A]=0\). Hence \(\mu\ll \mathbb{P}|_{\mathcal{G}}\).

By the Radonâ€“Nikodym theorem (Chapter 13), there exists a \(\mathcal{G}\)-measurable function \(Z\in L^1\) such that for all \(A\in\mathcal{G}\),
\[
\mu(A) = \int_A Z\, d\mathbb{P}.
\]
Equivalently,
\[
\mathbb{E}[Y\mathbf{1}_A] = \mathbb{E}[Z\mathbf{1}_A] \quad \forall\, A\in\mathcal{G}.
\]
We define \(Z := \mathbb{E}[Y\mid\mathcal{G}]\). Uniqueness holds \(\mathbb{P}\)-a.s. by uniqueness of the Radonâ€“Nikodym derivative.

### 14.3.2 Interpretation

Conditional expectation is not "probability with a denominator." It is the unique \(\mathcal{G}\)-measurable representative whose integrals match those of \(Y\) over all events visible to \(\mathcal{G}\). The object is therefore intrinsically informational: it is the best \(\mathcal{G}\)-compatible collapse of \(Y\).

<div class="ml-box">

In high-dimensional prediction, \(\mathcal{G}\) is the \(\sigma\)-algebra generated by whatever the model can read (pixels, tokens, sensor stream, or a learned representation \(h(X)\)). The "correct" target under that information horizon is not \(Y\) itself, but \(\mathbb{E}[Y\mid\mathcal{G}]\). Any attempt to produce predictions sharper than \(\mathcal{G}\) allows is mathematically illegal: it would contradict measurability.

</div>

---

## 14.4 Standard Properties of Conditional Expectation

<div class="prop-box">

Let \(Y,Y_1,Y_2\in L^1\), \(\mathcal{G}\subseteq\mathcal{H}\subseteq\mathcal{F}\), and \(Z\) be \(\mathcal{G}\)-measurable with \(ZY\in L^1\).

1. **Linearity.** \(\mathbb{E}[aY_1+bY_2\mid\mathcal{G}] = a\,\mathbb{E}[Y_1\mid\mathcal{G}]+b\,\mathbb{E}[Y_2\mid\mathcal{G}]\) a.s.
2. **Positivity.** \(Y\ge 0\) a.s. implies \(\mathbb{E}[Y\mid\mathcal{G}]\ge 0\) a.s.
3. **Tower property.** \(\mathbb{E}[\mathbb{E}[Y\mid\mathcal{H}]\mid\mathcal{G}] = \mathbb{E}[Y\mid\mathcal{G}]\) a.s.
4. **Pull-out rule.** \(\mathbb{E}[ZY\mid\mathcal{G}] = Z\,\mathbb{E}[Y\mid\mathcal{G}]\) a.s. (when \(Z\) is \(\mathcal{G}\)-measurable).
5. **Independence.** If \(Y\perp\!\!\!\perp\mathcal{G}\), then \(\mathbb{E}[Y\mid\mathcal{G}]=\mathbb{E}[Y]\) a.s.

</div>

<div class="ml-box">

**Tower property in ML.** In multi-stage models (encoder \(\to\) bottleneck \(\to\) decoder), if the encoder computes a representation \(h(X)\) and the decoder operates on \(h(X)\), the tower property guarantees that the expected loss evaluated at each stage is consistent: \(\mathbb{E}[\mathbb{E}[Y\mid h(X)]\mid X] = \mathbb{E}[Y\mid h(X)]\) when \(h(X)\) is a function of \(X\). Information can only be lost, never gained, by compressing through a bottleneck â€?this is the tower property as an information-theoretic identity.

</div>

---

## 14.5 Orthogonal Projection Theorem: MSE Minimization Is Projection in \(L^{2}\)

> **Theorem-sentence 14.2.** In the Hilbert space \(L^{2}(\Omega,\mathcal{F},\mathbb{P})\), the conditional expectation \(\mathbb{E}[Y\mid\mathcal{G}]\) is the orthogonal projection of \(Y\) onto the closed subspace \(L^{2}(\mathcal{G})\).

Assume \(Y\in L^{2}\). Let
\[
\mathcal{H} := L^{2}(\Omega,\mathcal{F},\mathbb{P}), \qquad \mathcal{H}_{\mathcal{G}} := L^{2}(\Omega,\mathcal{G},\mathbb{P})\subseteq \mathcal{H}.
\]
\(\mathcal{H}\) is a Hilbert space with inner product \(\langle U,V\rangle = \mathbb{E}[UV]\). \(\mathcal{H}_{\mathcal{G}}\) is a closed linear subspace.

Let \(\hat{Y} := \mathbb{E}[Y\mid\mathcal{G}]\). We claim \(\hat{Y} = \Pi_{\mathcal{H}_{\mathcal{G}}}(Y)\), the orthogonal projection. It suffices to verify the orthogonality condition:
\[
\langle Y - \hat{Y},\, Z\rangle = 0 \quad \forall\, Z\in\mathcal{H}_{\mathcal{G}}.
\]

<div class="proof-box">

**Proof.** Take any \(Z\in L^{2}(\mathcal{G})\). Approximate \(Z\) in \(L^2\) by simple \(\mathcal{G}\)-measurable functions \(Z_n = \sum_k a_k \mathbf{1}_{A_k}\) with \(A_k\in\mathcal{G}\). For each indicator \(\mathbf{1}_{A}\),
\[
\mathbb{E}[(Y-\hat{Y})\mathbf{1}_A] = \mathbb{E}[Y\mathbf{1}_A] - \mathbb{E}[\hat{Y}\mathbf{1}_A] = 0
\]
by the defining property of conditional expectation. By linearity, \(\mathbb{E}[(Y-\hat{Y})Z_n]=0\) for all \(n\). Passing to the \(L^2\)-limit yields \(\mathbb{E}[(Y-\hat{Y})Z]=0\). Therefore \(Y-\hat{Y}\perp\mathcal{H}_{\mathcal{G}}\), so \(\hat{Y}\) is the orthogonal projection. \(\square\)

</div>

---

## 14.6 The Pythagorean Identity: Why MSELoss Is Literally Pythagoras

> **Theorem-sentence 14.3.** For any \(Z\in L^{2}(\mathcal{G})\),
\[
\mathbb{E}[(Y-Z)^2] = \mathbb{E}[(Y-\hat{Y})^2] + \mathbb{E}[(\hat{Y}-Z)^2], \qquad \hat{Y}=\mathbb{E}[Y\mid\mathcal{G}].
\]
This is the Pythagorean theorem in \(L^{2}\).

<div class="proof-box">

**Proof (orthogonality expansion).** Write \(Y-Z = (Y-\hat{Y})+(\hat{Y}-Z)\). Square and take expectation:
\[
\mathbb{E}[(Y-Z)^2] = \mathbb{E}[(Y-\hat{Y})^2] + \mathbb{E}[(\hat{Y}-Z)^2] + 2\,\mathbb{E}[(Y-\hat{Y})(\hat{Y}-Z)].
\]
Since \(\hat{Y}-Z\in L^{2}(\mathcal{G})\) and \(Y-\hat{Y}\perp L^{2}(\mathcal{G})\), the cross term is zero, yielding the identity. \(\square\)

</div>

### 14.6.1 Machine learning consequences (invariants, not slogans)

**1. Bayes-optimal regression under squared loss.**
Among all \(\mathcal{G}\)-measurable predictors \(f\), the unique minimizer of \(\mathbb{E}[(Y-f)^2]\) is \(f^{\ast} = \mathbb{E}[Y\mid\mathcal{G}]\). Therefore, "training a regressor with MSE" is an attempt to approximate the projection map \(Y\mapsto\Pi_{\mathcal{H}_{\mathcal{G}}}Y\).

**2. Irreducible error is conditional variance.**
The minimal achievable risk is
\[
\inf_{Z\in L^{2}(\mathcal{G})} \mathbb{E}[(Y-Z)^2] = \mathbb{E}[(Y-\hat{Y})^2] = \mathbb{E}[\mathrm{Var}(Y\mid\mathcal{G})].
\]
This is not an artifact of optimization; it is the geometric distance from truth to the information subspace.

**3. Biasâ€“variance decomposition as projection decomposition.**
If a learned model \(\tilde{f}\in L^{2}(\mathcal{G})\) is random (depends on a training set), then
\[
\mathbb{E}[(Y-\tilde{f})^2] = \mathbb{E}[(Y-\hat{Y})^2] + \mathbb{E}[(\hat{Y}-\tilde{f})^2],
\]
where the second term splits into estimation-error components once \(\tilde{f}\) is expanded around \(\mathbb{E}[\tilde{f}\mid\mathcal{G}]\). The first term is purely geometric noise: it remains even with infinite data and perfect optimization.

---

## 14.7 Regression Examples: The Projection View Is the Entire Story

> **Theorem-sentence 14.4.** Many "best estimators" in modern ML are conditional expectations under an appropriate \(\sigma\)-algebra; the algorithm is merely a parametrized approximation to that projection.

### 14.7.1 Linear regression and least squares

Let \(X\in\mathbb{R}^d\), \(Y\in\mathbb{R}\), and take \(\mathcal{G}=\sigma(X)\). The Bayes regressor under squared loss is \(f^{\ast}(X)=\mathbb{E}[Y\mid X]\). Linear regression restricts the hypothesis class to affine functions \(f_w(X)=w^\top X+b\), i.e., a smaller subspace \(\mathcal{S}\subset L^{2}(\sigma(X))\). Minimizing population MSE over \(\mathcal{S}\) is the orthogonal projection of \(Y\) onto \(\mathcal{S}\). The normal equations are the orthogonality conditions:
\[
\mathbb{E}[(Y-f_w(X))\,X] = 0, \qquad \mathbb{E}[Y-f_w(X)] = 0.
\]
The "derivatives" are bookkeeping; the geometry is the cause.

### 14.7.2 Denoising autoencoders and diffusion: the posterior mean is the MSE optimum

Let \(X_0\) be clean data and observe \(X_t = X_0 + \varepsilon\) (or a general diffusion corruption) where \(\varepsilon\) is noise. If you train a network \(g_\theta\) by MSE to predict \(X_0\) from \(X_t\),
\[
\min_\theta \mathbb{E}\big[\|X_0 - g_\theta(X_t)\|^2\big],
\]
then the population optimum is
\[
g^{\ast}(X_t) = \mathbb{E}[X_0\mid X_t],
\]
the conditional expectation â€?exactly the orthogonal projection of \(X_0\) onto the information horizon generated by the noisy observation. This is the precise probabilistic core beneath denoising-based generative modeling: the target function is a conditional expectation; training is projection.

<div class="ml-box">

A useful corollary: the "irreducible" denoising error is \(\mathbb{E}[\mathrm{Var}(X_0\mid X_t)]\), which depends on the corruption level \(t\). This explains, at the level of measure geometry, why strong noise makes denoising intrinsically ambiguous and forces iterative refinement (multi-step reverse processes).

</div>

---

## 14.8 Conditional Expectation in Discrete Form: Attention as an Empirical Conditional Mean

> **Theorem-sentence 14.5.** A softmax attention head computes an empirical conditional expectation of value vectors under a learned, query-dependent discrete probability measure.

Fix tokens \(i\) (query position) and \(j\) (key/value positions). One head forms weights
\[
\alpha_{ij} = \frac{\exp(s(q_i,k_j))}{\sum_{m}\exp(s(q_i,k_m))},
\]
with \(\sum_j \alpha_{ij}=1\) and \(\alpha_{ij}\ge 0\). The output is
\[
o_i = \sum_{j} \alpha_{ij}\, v_j.
\]
This is an expectation: define a discrete probability measure \(\pi_i\) on indices \(j\) by \(\pi_i(\{j\})=\alpha_{ij}\). Then
\[
o_i = \mathbb{E}_{j\sim\pi_i}[v_j].
\]
What is being conditioned on? The query \(q_i\) induces the measure \(\pi_i\). Therefore, the head is a conditional-mean operator: "given the current information state (query), produce the best \(L^2\)-predictor of a value vector under the induced attention measure." Multi-head attention is a direct sum of such conditional-mean operators computed in different learned coordinate systems.

Two structural observations follow.

**1. Why attention is stable.**
Because \(o_i\) is a convex combination of \(\{v_j\}\), it lives in their convex hull. This is a measure-theoretic contract: an expectation cannot leave the support of its measure.

**2. Where attention can fail.**
If the correct prediction requires extrapolation beyond the convex hull of existing values, a pure attention layer cannot achieve it without additional transformations (MLPs, residual pathways). This is a limitation imposed by the geometry of conditional expectation, not an architectural accident.

---

## 14.9 Beyond Squared Loss: Conditional Expectation Generalizes to Bregman Projections

> **Theorem-sentence 14.6.** Squared loss yields orthogonal projection; more general proper losses yield generalized projections characterized by orthogonality in a dual geometry.

The special role of MSE comes from the Hilbert structure of \(L^2\). If one replaces squared loss by another loss, the "best predictor" is no longer an orthogonal projection in the \(L^2\) metric, but it remains a projection in an information-geometric sense.

A concrete bridge to classification: let \(Y\in\{1,\dots,K\}\). If you minimize expected cross-entropy over all measurable predictors \(p(\cdot\mid X)\) on the simplex,
\[
\min_{p(\cdot\mid X)} \mathbb{E}[-\log p(Y\mid X)],
\]
the unique minimizer is the true conditional distribution:
\[
p^{\ast}(y\mid X) = \mathbb{P}(Y=y\mid X).
\]
Thus cross-entropy training is the "conditional expectation" appropriate to log-loss: not a scalar mean, but a conditional law. In modern ML terms: MSE targets posterior means; cross-entropy targets posterior probabilities. Both are dictated by the geometry of the loss.

<div class="ml-box">

This clarifies a frequent conceptual confusion: using MSE for classification forces the model toward conditional means of one-hot vectors (class probabilities), but with a metric mismatch that changes optimization geometry and calibration behavior. The two losses are not interchangeable approximations; they project onto the same subspace using different geometries, and their behaviors at the boundary of the simplex diverge sharply.

</div>

---

## 14.10 Scholium: What a Mathematician Would Actually Conclude

<div class="scholium-box">

1. **Conditional expectation is an RN derivative on an information horizon.** It exists because a certain measure is absolutely continuous, and it is unique a.e.

2. **MSE is the squared norm in \(L^2\), so minimization is projection.** Backpropagation is an algorithmic method for approximating the projection point inside a parametrized submanifold of \(L^2(\mathcal{G})\).

3. **Many deep learning primitives are conditional-mean operators under learned measures.** Attention is an explicit expectation under a query-induced discrete measure; denoising objectives target posterior means; domain reweighting is change-of-measure.

4. **The loss selects the geometry of projection.** Squared loss yields Hilbert orthogonality; log-loss yields probability-simplex geometry. The distinction matters for what the learned object represents (mean vs. full conditional law) and for how gradients behave.

</div>

This completes the geometric foundation of prediction under information constraints. Once prediction is understood as projection, the natural question is what global divergences and curvatures govern the space of probability measures. That is the domain of information geometry, KL divergence, and cross-entropy as the dominant training functional in large-scale models.

**Chapter 015: Information Geometry and Divergences â€?Curvature on the Manifold of Probability Measures.**
